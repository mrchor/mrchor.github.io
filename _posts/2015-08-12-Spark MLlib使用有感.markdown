---
layout:     post
title:      "Spark MLlib使用有感"
date:       2015-08-12 12:00:00
author:     "Mrchor"
header-img: "img/post-bg-2015.jpg"
catalog:	true
tags:	技术 Spark MLLib
---

> “这就是我，一个低调的做者。”



　　这些天在公司里面做文本分析的任务，我跟着玻哥一起做，先研究了算法的可行度，最后决定使用Google的Word2Vector和LDA算法来对文本进行分析。之前因为看过一些Spark的东西，所以准备瞄准MLlib，直接使用其机器学习库来进行算法的测试。

　　但是发现一个非常重大的问题——因为Spark默认是将RDD持久到内存中进行计算的，但是当我们加大数据量的时候，由于本集群的内存不是很够，导致每次在进行迭代的时候都出现heap溢出。我追踪了Spark给出的这两个算法的源码，发现是其算法内部给出的每次迭代的结果都以RDD持久在了内存中，导致溢出。

　　由于本人对于scala语言不是很熟悉，所以将此问题提交至github的spark相关核心成员，但是目前还没给出相应的答复。

　　得出一个结论，虽然现在的Spark如火如荼，但是其MLlib库在文本分析方面还是非常的鸡肋，暂时不是大数据文本分析的好的选择。