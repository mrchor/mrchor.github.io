---
layout:     post
title:      "干货！NAIS：一种基于注意力机制的神经网络item相似模型"
date:       2020-5-24 08:00:00
author:     "Mrchor"
header-img: "img/post-bg-2015.jpg"
catalog:	true
tags: 个性化推荐 计算广告 推荐业务
---

> “周而复始不断挑战自我，杰出己身切勿闭门造车。”

<br>
[](/img/2020-5-24/0.jpg)
<br>

## 前言
在为用户推荐时，我们无法将海量item数据挨个进行排序直接选取topN个item给用户，因此我们需要使用一些策略或模型先从海量数据中筛选若干可控数目的item，然后再进行精确排序，而这个过程就称之为召回。召回池的好坏直接决定了我们给用户展示的item内容的优劣性，所以可以说，召回对于我们整个推荐架构也是至关重要的一环。

我们常用的召回主要分类两大部分，一是策略类召回，二是模型类召回。策略类召回，我们暂且不谈，模型类召回有很多，例如YouTubeNet、还有我们今天的主角——协同过滤。最经典的算法莫过于基于item的协同过滤（item-based CF），我们通过用户已知历史行为的item，可以向用户推荐该item类似的item。

基于item的协同过滤主要是在计算item的相似度。但是早期的方法使用统计量例如余弦相似度、皮尔逊系数等来估计item相似度的，这其实是不准确的，因为它缺少对推荐任务的定制化，即无法根据实际问题"因地制宜"，例如基于item的协同过滤很有可能受平台中活跃用户的影响而使得item相似度计算更偏向于热门item。最近几年，一些团队试图从数据中学习item的相似性，将相似性表示为基础模型，并通过优化一个支持推荐的目标函数来估计模型参数，但这也仅仅局限在使用浅层线性模型来学习物品相似度，对于探索item之间非线性神经网络的研究相对较少。

本文的工作成果命名为NAIS——基于注意力机制的神经网络item相似模型。在模型设计中，**使用注意力机制分辨出那些对未来预测更重要的用户历史行为的item**，并且已经验证了比传统的基于item的协同过滤更有效果，模型表达能力更强。

##一、传统的基于item的协同过滤
传统的基于item的协同过滤的思想是利用用户历史交互行为的item集合预测用户对目标item的行为，其公式表达如下：

![](/img/2020-5-24/1.jpg)

其中，R+u表示用户u的历史交互行为item集合，Sij表示item-i与item-j之间的相似度，ruj表示用户对item-j的已知偏好程度（可以是显式的偏好分计算，也可以是隐式的用户点击1或曝光行为0）。

我们在做线上推荐时，主要是通过已知的item之间的相似度，进而根据上述公式预测出用户u对item-i的行为，那么如何计算item之间的相似度呢，其实这包含两大类方法，一种是基于余弦相似度或皮尔逊系数来衡量；另外一种是在用户-item交互图中使用随机游走（random walk）。但是这种启发式算法模型缺乏对实际推荐场景的定制化，可能会导致陷入次优化的问题。

注：笔者认为此处次优化问题，类似于ALS（Alternating Least Square，交替最小二乘），另外一种召回模型，交替计算用户矩阵和item矩阵，但最终也无法计算出最优解，只能逼近。

## 二、基于注意力机制的神经网络item相似模型

![](/img/2020-5-24/2.jpg)

上图就是基于NAIS模型的协同过滤框架图，由图可以看出NAIS的思想，即在传统的基于item的协同过滤中加入了注意力网络用以调节用户历史行为的items对于预测的用户对item-i的行为中，其所起到的权重作用是不一样的，这里甚至可以得出一个观点，即：**用户中远期历史行为的item对当前的预测权重值不一定小于用户近期历史行为的item对当前的预测权重值。**这对我们之后在做用户偏好计算时有一定的借鉴意义。以下是作者设计的预测模型方程：

![](/img/2020-5-24/3.jpg)

其中aj是一个可训练参数，用以表征item-j在用户u的贡献权重。

虽然这个模型似乎能够区分历史行为item的重要性，但是它忽略了目标item对历史item的影响。特别是，作者认为，为所有预测指定一个历史行为item的全局权重是不合理的。例如，在为用户推荐手机和衣服时，用户历史行为的item集合中，aj对于推荐手机或衣服，它的值是恒定不变的，这其实是不合理的，因此作者对上述模型进行了一次改进，改进后的模型方程为：

![](/img/2020-5-24/4.jpg)

通过将aj变换为aij，可以使我们在推荐时，对于为用户推荐手机和衣服时，不同的权重aij会使我们的推荐结果有一定的差异性而得到了进一步的合理性。

但是上述公式中，我们无法在训练模型时穷举所有aij的情况，这就使模型泛化能力大打折扣，因此，作者设计了向量pi和qj，用它们的函数来表示aij以增强模型的泛化能力：

![](/img/2020-5-24/5.jpg)

此处作者设计了两种函数用来表征aij，如下：

![](/img/2020-5-24/6.jpg)

上述函数的区别很小，第一种函数内部的两个向量做了合并处理，第二种函数内部的两个向量做了内积处理。其中激活函数均使用了ReLU。此时得到的预测模型方程如下：

![](/img/2020-5-24/7.jpg)

然而上述经过泛化表征后的模型由于以softmax表示的注意力机制的使用，使得aij的权重值很小，这可能会过度惩罚历史行为时间较长的活跃用户的权重。如下图，以MovieLens数据为例，左子图为例，所有用户的平均长度为166，而最大长度为2313。也就是说，最活跃的用户的平均注意力权重是1/2313，而所有用户平均注意力权重是1/166，也就是说所有用户人均平均注意力权重是最活跃用户注意力权重的14倍之多，这显然是不合理的。

![](/img/2020-5-24/8.jpg)

于是作者设计了带有平滑因子β的softmax的注意力机制，即如下模型公式：

![](/img/2020-5-24/9.jpg)

其中β是一个平滑因子超参，当其小于1.0时，抑制了分母的大小，从而减轻了对活跃用户历史行为注意力权重的惩罚。

本文模型构建中，最终的损失函数设计为一个类似于传统二分类模型的损失函数，即：

![](/img/2020-5-24/10.jpg)

其中，y_hat ui表示用户是否点击。

## 三、实验结论

![](/img/2020-5-24/11.jpg)

如图可以看出，NAIS模型在准确率均超过了其他模型，并且可以看出随着embedding_size的增长，模型的表达能力有一定的提升，但同时也带来了模型复杂度的急剧提升。

## 总结
NAIS模型的设计一定程度上借鉴了CV或NLP中的注意力机制，从而提升了在推荐场景中召回池的效果，但是其参数设置之多，导致其训练迭代会很慢，并且笔者认为模型中的β设计较为简单粗暴，即超参β设置从一定程度上引入了人为的因素，是否可以将其作为一个参数，直接在模型中训练比较好？当然，这可能进一步带来模型训练的复杂性，但是由于召回模型是在离线训练，线上提供召回服务，因此这一块的担忧，笔者认为可以忽略不计。

如果大家喜欢我的文章，可以关注我的公众号：软客圈（ID：recoquan）~

附：

论文链接：https://arxiv.org/pdf/1809.07053.pdf

论文实现：https://github.com/AaronHeee/Neural-Attentive-Item-Similarity-Model




#**注：纯手工打造，实属不易，欢迎大家分享和转发~**
#**原创内容，转载需注明出处，否则视为侵权并将被追诉！**

